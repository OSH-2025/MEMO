"""
æµ‹è¯•ç‰ˆå·¥ä½œæµ - ä¸éœ€è¦çœŸå®æ¨¡å‹ï¼Œå¯ä»¥éªŒè¯æ•°æ®æµå’Œé˜Ÿåˆ—é€»è¾‘
"""

import os
import sys
import time
import json
import threading
import datetime
from typing import Dict, Any, Optional, List
from collections import deque

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data_collection'))


class MockLLMPredictor:
    """æ¨¡æ‹ŸLLMé¢„æµ‹å™¨ - ç”¨äºæµ‹è¯•"""
    
    def __init__(self, model_path: str = None, lora_path: str = None):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿé¢„æµ‹å™¨"""
        print("ğŸ¤– ä½¿ç”¨æ¨¡æ‹ŸLLMé¢„æµ‹å™¨ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        self.model_path = model_path
        self.lora_path = lora_path
        
        # é¢„å®šä¹‰çš„æµ‹è¯•é¢„æµ‹ç»“æœ
        self.mock_predictions = [
            "2025-05-22 08:31:15 - è®¿é—®ç½‘ç«™ github.com çš„é¡µé¢ 'MEMO/README.md at main Â· OSH-2025/MEMO'",
            "2025-05-22 08:31:45 - åˆ‡æ¢åˆ°åº”ç”¨ VSCode - 'main.py'",
            "2025-05-22 08:32:10 - è®¿é—®ç½‘ç«™ stackoverflow.com çš„é¡µé¢ 'Python threading tutorial'",
            "2025-05-22 08:32:35 - è®¿é—®ç½‘ç«™ docs.python.org çš„é¡µé¢ 'Queue objects'",
            "2025-05-22 08:33:00 - åˆ‡æ¢åˆ°åº”ç”¨ Chrome - 'GitHub'",
        ]
        self.prediction_index = 0
    
    def predict_next_activity(self, activity_sequence: str) -> str:
        """æ¨¡æ‹Ÿé¢„æµ‹ä¸‹ä¸€ä¸ªæ´»åŠ¨"""
        print(f"ğŸ“ è¾“å…¥æ´»åŠ¨åºåˆ—é•¿åº¦: {len(activity_sequence.split(chr(10)))} è¡Œ")
        
        # è¿”å›æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
        prediction = self.mock_predictions[self.prediction_index % len(self.mock_predictions)]
        self.prediction_index += 1
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§
        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        prediction = prediction.replace('2025-05-22 08:3', current_time[:16])
        
        return prediction


class ActivityQueue:
    """æ´»åŠ¨é˜Ÿåˆ—ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
    
    def __init__(self, max_size: int = 20, history_window: int = 100):
        self.max_size = max_size
        self.history_window = history_window
        self.activity_history = deque(maxlen=history_window)
        self.total_activities = 0
        self.last_activity_time = None
        
        print(f"ğŸ“¦ æ´»åŠ¨é˜Ÿåˆ—åˆå§‹åŒ–å®Œæˆ - é¢„æµ‹çª—å£: {max_size}, å†å²çª—å£: {history_window}")
    
    def add_activity(self, activity: Dict[str, Any]):
        """æ·»åŠ æ–°æ´»åŠ¨åˆ°é˜Ÿåˆ—"""
        if 'timestamp' not in activity:
            activity['timestamp'] = datetime.datetime.now().isoformat()
        
        self.activity_history.append(activity)
        self.total_activities += 1
        self.last_activity_time = activity['timestamp']
        
        print(f"â• æ·»åŠ æ´»åŠ¨: {activity.get('type', 'unknown')} - {activity.get('domain', activity.get('process_name', 'N/A'))}")
    
    def get_recent_activities(self, count: Optional[int] = None) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„æ´»åŠ¨åºåˆ—"""
        if count is None:
            count = self.max_size
        
        recent_count = min(count, len(self.activity_history))
        if recent_count == 0:
            return []
        
        return list(self.activity_history)[-recent_count:]
    
    def format_activities_for_llm(self, activities: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ´»åŠ¨åºåˆ—ä¸ºæ–‡æœ¬"""
        formatted_activities = []
        
        for activity in activities:
            formatted = self._format_single_activity(activity)
            if formatted:
                formatted_activities.append(formatted)
        
        return '\n'.join(formatted_activities)
    
    def _format_single_activity(self, activity: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ´»åŠ¨"""
        activity_type = activity.get('type', '')
        timestamp = activity.get('timestamp', '')
        
        try:
            dt = datetime.datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            formatted_time = dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            formatted_time = timestamp
        
        if activity_type == 'browser_history':
            domain = activity.get('domain', '')
            title = activity.get('title', '')
            if domain and title:
                return f"{formatted_time} - è®¿é—®ç½‘ç«™ {domain} çš„é¡µé¢ '{title}'"
            elif domain:
                return f"{formatted_time} - è®¿é—®ç½‘ç«™ {domain}"
        
        elif activity_type == 'window_focus':
            process_name = activity.get('process_name', '')
            window_title = activity.get('window_title', '')
            if process_name and window_title:
                return f"{formatted_time} - åˆ‡æ¢åˆ°åº”ç”¨ {process_name} - '{window_title}'"
        
        return f"{formatted_time} - {activity_type} æ´»åŠ¨"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_activities': self.total_activities,
            'current_history_size': len(self.activity_history),
            'last_activity_time': self.last_activity_time,
            'queue_size': len(self.activity_history)
        }


class TestActivityPipeline:
    """æµ‹è¯•ç‰ˆæ´»åŠ¨é¢„æµ‹ç®¡é“"""
    
    def __init__(self, 
                 model_path: str = "mock",
                 lora_path: str = "mock", 
                 queue_size: int = 20,
                 prediction_interval: int = 30):
        self.prediction_interval = prediction_interval
        
        print("ğŸ§ª åˆå§‹åŒ–æµ‹è¯•ç‰ˆæ´»åŠ¨é¢„æµ‹ç®¡é“...")
        self.activity_queue = ActivityQueue(max_size=queue_size)
        self.llm_predictor = MockLLMPredictor(model_path, lora_path)
        
        self.running = False
        self.prediction_thread = None
        self.prediction_history = deque(maxlen=50)
        
        print("âœ… æµ‹è¯•ç‰ˆæ´»åŠ¨é¢„æµ‹ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def add_activity(self, activity: Dict[str, Any]):
        """æ·»åŠ æ–°æ´»åŠ¨"""
        self.activity_queue.add_activity(activity)
    
    def start_prediction_loop(self):
        """å¯åŠ¨é¢„æµ‹å¾ªç¯"""
        if self.running:
            print("ğŸ”„ é¢„æµ‹å¾ªç¯å·²åœ¨è¿è¡Œ")
            return
        
        self.running = True
        self.prediction_thread = threading.Thread(target=self._prediction_loop, daemon=True)
        self.prediction_thread.start()
        print("ğŸš€ é¢„æµ‹å¾ªç¯å·²å¯åŠ¨")
    
    def stop_prediction_loop(self):
        """åœæ­¢é¢„æµ‹å¾ªç¯"""
        self.running = False
        if self.prediction_thread:
            self.prediction_thread.join(timeout=5)
        print("â¹ï¸  é¢„æµ‹å¾ªç¯å·²åœæ­¢")
    
    def _prediction_loop(self):
        """é¢„æµ‹å¾ªç¯ä¸»å‡½æ•°"""
        while self.running:
            try:
                recent_activities = self.activity_queue.get_recent_activities()
                
                if len(recent_activities) >= 3:
                    activity_sequence = self.activity_queue.format_activities_for_llm(recent_activities)
                    prediction = self.llm_predictor.predict_next_activity(activity_sequence)
                    
                    prediction_record = {
                        'timestamp': datetime.datetime.now().isoformat(),
                        'input_activities': recent_activities[-3:],
                        'prediction': prediction,
                        'activity_count': len(recent_activities)
                    }
                    
                    self.prediction_history.append(prediction_record)
                    
                    print(f"\nğŸ”® [{datetime.datetime.now().strftime('%H:%M:%S')}] æ´»åŠ¨é¢„æµ‹:")
                    print(f"ğŸ“Š åŸºäºæœ€è¿‘ {len(recent_activities)} ä¸ªæ´»åŠ¨çš„é¢„æµ‹:")
                    print(f"ğŸ¯ é¢„æµ‹ç»“æœ: {prediction}")
                    print("-" * 60)
                
                else:
                    print(f"â³ [{datetime.datetime.now().strftime('%H:%M:%S')}] æ´»åŠ¨æ•°æ®ä¸è¶³ï¼ˆå½“å‰: {len(recent_activities)}/3ï¼‰ï¼Œç­‰å¾…æ›´å¤šæ•°æ®...")
                
                time.sleep(self.prediction_interval)
                
            except Exception as e:
                print(f"âŒ é¢„æµ‹å¾ªç¯é”™è¯¯: {e}")
                time.sleep(5)
    
    def get_latest_prediction(self) -> Optional[Dict[str, Any]]:
        """è·å–æœ€æ–°é¢„æµ‹ç»“æœ"""
        if self.prediction_history:
            return self.prediction_history[-1]
        return None
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """è·å–ç®¡é“çŠ¶æ€"""
        queue_stats = self.activity_queue.get_stats()
        
        return {
            'running': self.running,
            'queue_stats': queue_stats,
            'prediction_count': len(self.prediction_history),
            'latest_prediction_time': self.prediction_history[-1]['timestamp'] if self.prediction_history else None
        }


def demo_test_workflow():
    """æ¼”ç¤ºæµ‹è¯•å·¥ä½œæµ"""
    print("=" * 60)
    print("ğŸ§ª å¯åŠ¨æµ‹è¯•ç‰ˆå®æ—¶æ´»åŠ¨é¢„æµ‹æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•ç®¡é“
    pipeline = TestActivityPipeline(prediction_interval=10)
    
    # ç¤ºä¾‹æ´»åŠ¨æ•°æ®
    test_activities = [
        {
            'type': 'browser_history',
            'domain': 'www.douban.com',
            'title': 'ã€ç›˜ä¸ªå‰§æœ¬æŠ¼ä¸ªCã€‘ã€Game of Thronesã€‘1st episode ã€ŠAll men must dieã€‹'
        },
        {
            'type': 'browser_history', 
            'domain': 'github.com',
            'title': 'self-llm/examples/AMchat-é«˜ç­‰æ•°å­¦ at master Â· datawhalechina/self-llm'
        },
        {
            'type': 'browser_history',
            'domain': 'github.com', 
            'title': 'MEMO/feasibility_report/Fine-tuning of LLM.md at main Â· OSH-2025/MEMO'
        },
        {
            'type': 'window_focus',
            'process_name': 'Code.exe',
            'window_title': 'realtime_pipeline.py - Visual Studio Code'
        },
        {
            'type': 'browser_history',
            'domain': 'github.com',
            'title': 'MEMO/src at main Â· OSH-2025/MEMO'
        }
    ]
    
    try:
        # å¯åŠ¨é¢„æµ‹å¾ªç¯
        pipeline.start_prediction_loop()
        
        print("ğŸ“ é€æ­¥æ·»åŠ æµ‹è¯•æ´»åŠ¨æ•°æ®...\n")
        
        # é€æ­¥æ·»åŠ æ´»åŠ¨
        for i, activity in enumerate(test_activities, 1):
            print(f"ğŸ“Œ æ·»åŠ ç¬¬ {i} ä¸ªæ´»åŠ¨:")
            pipeline.add_activity(activity)
            
            # æ˜¾ç¤ºå½“å‰çŠ¶æ€
            status = pipeline.get_pipeline_status()
            print(f"ğŸ“ˆ å½“å‰é˜Ÿåˆ—ä¸­æœ‰ {status['queue_stats']['current_history_size']} ä¸ªæ´»åŠ¨")
            
            print("-" * 30)
            time.sleep(3)
        
        print("\nğŸ”„ ç­‰å¾…è‡ªåŠ¨é¢„æµ‹ç»“æœ...")
        print("ç³»ç»Ÿå°†æ¯10ç§’è‡ªåŠ¨è¿›è¡Œä¸€æ¬¡é¢„æµ‹\n")
        
        # ç­‰å¾…å¹¶æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        for i in range(5):
            time.sleep(10)
            latest = pipeline.get_latest_prediction()
            if latest:
                print(f"ğŸ†• [è‡ªåŠ¨é¢„æµ‹ {i+1}] {latest['timestamp']}")
                print(f"ğŸ¯ é¢„æµ‹: {latest['prediction']}")
                print()
            else:
                print(f"â³ [è‡ªåŠ¨é¢„æµ‹ {i+1}] ç­‰å¾…é¢„æµ‹ç»“æœ...")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    
    finally:
        pipeline.stop_prediction_loop()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        final_status = pipeline.get_pipeline_status()
        print("\n" + "=" * 60)
        print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»æ´»åŠ¨æ•°: {final_status['queue_stats']['total_activities']}")
        print(f"   é¢„æµ‹æ¬¡æ•°: {final_status['prediction_count']}")
        print(f"   é˜Ÿåˆ—å¤§å°: {final_status['queue_stats']['current_history_size']}")
        print("=" * 60)
        print("ğŸ‰ æµ‹è¯•æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•ç‰ˆå®æ—¶æ´»åŠ¨é¢„æµ‹ç³»ç»Ÿ")
    print("è¿™ä¸ªç‰ˆæœ¬ä¸éœ€è¦çœŸå®çš„LLMæ¨¡å‹ï¼Œå¯ä»¥éªŒè¯å·¥ä½œæµé€»è¾‘")
    print()
    
    try:
        demo_test_workflow()
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 