"""
LinuxæœåŠ¡å™¨ - æ¥æ”¶Windowså®¢æˆ·ç«¯æ•°æ®ï¼Œè¿è¡ŒLLMé¢„æµ‹
"""

import os
import sys
import time
import json
import uuid
import asyncio
import threading
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from collections import defaultdict, deque

# Webæ¡†æ¶
from aiohttp import web, WSMsgType
import aiohttp_cors

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from client_server_architecture import (
    ActivityData, PredictionRequest, PredictionResponse,
    ActivityBuffer, ServerConfig, format_activities_for_llm, logger
)

# LLMç›¸å…³å¯¼å…¥
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    from peft import PeftModel
    LLM_LIBS_AVAILABLE = True
except ImportError:
    logger.warning("LLM libraries not available, using mock predictor")
    LLM_LIBS_AVAILABLE = False

class LLMPredictor:
    """LLMé¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, lora_path: str, enable_gpu: bool = True):
        self.model_path = model_path
        self.lora_path = lora_path
        self.enable_gpu = enable_gpu
        
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        logger.info(f"ğŸ¤– LLMé¢„æµ‹å™¨åˆå§‹åŒ– - GPU: {enable_gpu}")
    
    async def load_model(self):
        """å¼‚æ­¥åŠ è½½æ¨¡å‹"""
        if not LLM_LIBS_AVAILABLE:
            logger.warning("ğŸš« LLMåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿé¢„æµ‹å™¨")
            self.is_loaded = True
            return True
        
        try:
            logger.info("ğŸ“¥ å¼€å§‹åŠ è½½LLMæ¨¡å‹...")
            
            # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ä»¥é¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model_sync)
            
            self.is_loaded = True
            logger.info("âœ… LLMæ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _load_model_sync(self):
        """åŒæ­¥åŠ è½½æ¨¡å‹"""
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        device_map = "auto" if self.enable_gpu else "cpu"
        torch_dtype = torch.bfloat16 if self.enable_gpu else torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map=device_map,
            torch_dtype=torch_dtype,
            trust_remote_code=True
        ).eval()
        
        # åŠ è½½LoRAæƒé‡
        if os.path.exists(self.lora_path):
            self.model = PeftModel.from_pretrained(self.model, model_id=self.lora_path)
            logger.info(f"ğŸ“¦ LoRAæƒé‡å·²åŠ è½½: {self.lora_path}")
    
    async def predict_next_activity(self, activities: List[ActivityData]) -> Dict[str, Any]:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªæ´»åŠ¨"""
        start_time = time.time()
        
        if not self.is_loaded:
            return self._mock_prediction(activities, start_time)
        
        if not LLM_LIBS_AVAILABLE:
            return self._mock_prediction(activities, start_time)
        
        try:
            # æ ¼å¼åŒ–æ´»åŠ¨åºåˆ—
            activity_sequence = format_activities_for_llm(activities)
            
            # æ„é€ prompt
            system_prompt = (
                "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·æœ€è¿‘çš„æ´»åŠ¨åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½çš„ç”¨æˆ·æ´»åŠ¨ã€‚"
                "è¾“å‡ºæ ¼å¼ç±»ä¼¼\"2025-05-22 08:30:38 - è®¿é—®ç½‘ç«™ github.com çš„é¡µé¢ 'README.md'\"ï¼Œ"
                "æ³¨æ„è¿™é‡Œçš„ç½‘é¡µæˆ–åº”ç”¨åªè€ƒè™‘å·²çŸ¥ç¡®å®šçš„ç½‘é¡µå’Œåº”ç”¨ã€‚\n"
                "<|eot_id|>"
            )
            
            user_prompt = (
                "<|start_header_id|>user<|end_header_id|>\n\n"
                "æ ¹æ®ç”¨æˆ·ä¹‹å‰çš„æ´»åŠ¨åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„æ´»åŠ¨ã€‚\n"
                "ç”¨æˆ·æ´»åŠ¨åºåˆ—:\n"
                f"{activity_sequence}\n"
                "<|eot_id|>"
            )
            
            assistant_prompt = "<|start_header_id|>assistant<|end_header_id|>\n\n"
            
            full_prompt = system_prompt + user_prompt + assistant_prompt
            
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¨ç†
            loop = asyncio.get_event_loop()
            prediction = await loop.run_in_executor(
                None, 
                self._generate_prediction, 
                full_prompt
            )
            
            processing_time = time.time() - start_time
            
            return {
                'prediction': prediction.strip(),
                'confidence': 0.85,  # æš‚æ—¶ç¡¬ç¼–ç 
                'processing_time': processing_time,
                'model_used': 'llama3-instruct-lora'
            }
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return self._mock_prediction(activities, start_time)
    
    def _generate_prediction(self, prompt: str) -> str:
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        # è½¬token id
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=128,
                temperature=0.7,
                do_sample=True,
                top_p=0.95,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # è§£ç ï¼Œä»…å–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = self.tokenizer.decode(
            generated_ids[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return response
    
    def _mock_prediction(self, activities: List[ActivityData], start_time: float) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ"""
        mock_predictions = [
            "2025-05-22 08:31:15 - è®¿é—®ç½‘ç«™ github.com çš„é¡µé¢ 'MEMO/README.md'",
            "2025-05-22 08:31:45 - åˆ‡æ¢åˆ°åº”ç”¨ VSCode - 'main.py'",
            "2025-05-22 08:32:10 - è®¿é—®ç½‘ç«™ stackoverflow.com çš„é¡µé¢ 'Python async tutorial'",
            "2025-05-22 08:32:35 - è®¿é—®ç½‘ç«™ docs.python.org çš„é¡µé¢ 'asyncio documentation'",
        ]
        
        # æ ¹æ®æœ€è¿‘æ´»åŠ¨é€‰æ‹©é¢„æµ‹
        if activities:
            last_activity = activities[-1]
            if last_activity.activity_type == 'browser_history':
                prediction = mock_predictions[0]
            elif last_activity.activity_type == 'window_focus':
                prediction = mock_predictions[1]
            else:
                prediction = mock_predictions[2]
        else:
            prediction = mock_predictions[0]
        
        # æ›´æ–°æ—¶é—´æˆ³ä¸ºå½“å‰æ—¶é—´
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        prediction = prediction.replace('2025-05-22 08:3', current_time[:16])
        
        processing_time = time.time() - start_time
        
        return {
            'prediction': prediction,
            'confidence': 0.75,
            'processing_time': processing_time,
            'model_used': 'mock-predictor'
        }

class ClientManager:
    """å®¢æˆ·ç«¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.clients: Dict[str, Dict[str, Any]] = {}
        self.client_activities: Dict[str, ActivityBuffer] = {}
        self.lock = threading.Lock()
        
        logger.info("ğŸ‘¥ å®¢æˆ·ç«¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def register_client(self, client_id: str) -> bool:
        """æ³¨å†Œæ–°å®¢æˆ·ç«¯"""
        with self.lock:
            if client_id not in self.clients:
                self.clients[client_id] = {
                    'id': client_id,
                    'connected_at': datetime.now().isoformat(),
                    'last_heartbeat': datetime.now().isoformat(),
                    'activities_count': 0,
                    'predictions_count': 0
                }
                
                self.client_activities[client_id] = ActivityBuffer(max_size=200)
                
                logger.info(f"ğŸ“± æ–°å®¢æˆ·ç«¯æ³¨å†Œ: {client_id}")
                return True
            
            return False
    
    def update_heartbeat(self, client_id: str) -> bool:
        """æ›´æ–°å®¢æˆ·ç«¯å¿ƒè·³"""
        with self.lock:
            if client_id in self.clients:
                self.clients[client_id]['last_heartbeat'] = datetime.now().isoformat()
                return True
            return False
    
    def add_activities(self, client_id: str, activities: List[ActivityData]) -> bool:
        """æ·»åŠ å®¢æˆ·ç«¯æ´»åŠ¨æ•°æ®"""
        with self.lock:
            if client_id in self.client_activities:
                buffer = self.client_activities[client_id]
                for activity in activities:
                    buffer.add_activity(activity)
                
                self.clients[client_id]['activities_count'] += len(activities)
                logger.debug(f"ğŸ“ å®¢æˆ·ç«¯ {client_id} æ·»åŠ äº† {len(activities)} ä¸ªæ´»åŠ¨")
                return True
            
            return False
    
    def get_client_activities(self, client_id: str, count: int = 20) -> List[ActivityData]:
        """è·å–å®¢æˆ·ç«¯çš„æœ€è¿‘æ´»åŠ¨"""
        with self.lock:
            if client_id in self.client_activities:
                return self.client_activities[client_id].get_recent_activities(count)
            return []
    
    def increment_prediction_count(self, client_id: str):
        """å¢åŠ é¢„æµ‹è®¡æ•°"""
        with self.lock:
            if client_id in self.clients:
                self.clients[client_id]['predictions_count'] += 1
    
    def get_client_info(self, client_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å®¢æˆ·ç«¯ä¿¡æ¯"""
        with self.lock:
            return self.clients.get(client_id)
    
    def get_all_clients(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å®¢æˆ·ç«¯ä¿¡æ¯"""
        with self.lock:
            return list(self.clients.values())
    
    def cleanup_inactive_clients(self, timeout_minutes: int = 10):
        """æ¸…ç†ä¸æ´»è·ƒçš„å®¢æˆ·ç«¯"""
        cutoff_time = datetime.now() - timedelta(minutes=timeout_minutes)
        
        with self.lock:
            inactive_clients = []
            
            for client_id, client_info in self.clients.items():
                last_heartbeat = datetime.fromisoformat(client_info['last_heartbeat'])
                if last_heartbeat < cutoff_time:
                    inactive_clients.append(client_id)
            
            for client_id in inactive_clients:
                del self.clients[client_id]
                del self.client_activities[client_id]
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä¸æ´»è·ƒå®¢æˆ·ç«¯: {client_id}")

class ActivityPredictionServer:
    """æ´»åŠ¨é¢„æµ‹æœåŠ¡å™¨"""
    
    def __init__(self, config: ServerConfig):
        self.config = config
        self.client_manager = ClientManager()
        self.llm_predictor = LLMPredictor(
            config.model_path, 
            config.lora_path, 
            config.enable_gpu
        )
        
        self.app = web.Application()
        self.setup_routes()
        self.setup_cors()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'total_requests': 0,
            'total_predictions': 0,
            'total_activities': 0,
            'active_clients': 0
        }
        
        logger.info(f"ğŸ–¥ï¸ æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ - ç«¯å£: {config.port}")
    
    def setup_cors(self):
        """è®¾ç½®CORS"""
        cors = aiohttp_cors.setup(self.app, defaults={
            "*": aiohttp_cors.ResourceOptions(
                allow_credentials=True,
                expose_headers="*",
                allow_headers="*",
                allow_methods="*"
            )
        })
        
        # ä¸ºæ‰€æœ‰è·¯ç”±æ·»åŠ CORS
        for route in list(self.app.router.routes()):
            cors.add(route)
    
    def setup_routes(self):
        """è®¾ç½®è·¯ç”±"""
        self.app.router.add_get('/health', self.health_check)
        self.app.router.add_get('/status', self.get_status)
        self.app.router.add_get('/clients', self.get_clients)
        
        self.app.router.add_post('/heartbeat', self.handle_heartbeat)
        self.app.router.add_post('/activity', self.handle_single_activity)
        self.app.router.add_post('/activities/batch', self.handle_batch_activities)
        self.app.router.add_post('/predict', self.handle_prediction_request)
        
        logger.info("ğŸ”— HTTPè·¯ç”±è®¾ç½®å®Œæˆ")
    
    async def health_check(self, request):
        """å¥åº·æ£€æŸ¥"""
        return web.json_response({
            'status': 'healthy',
            'server': 'activity-prediction-server',
            'timestamp': datetime.now().isoformat(),
            'llm_loaded': self.llm_predictor.is_loaded
        })
    
    async def get_status(self, request):
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        self.stats['active_clients'] = len(self.client_manager.clients)
        
        return web.json_response({
            'stats': self.stats,
            'config': self.config.to_dict(),
            'llm_status': {
                'loaded': self.llm_predictor.is_loaded,
                'model_path': self.config.model_path,
                'lora_path': self.config.lora_path
            }
        })
    
    async def get_clients(self, request):
        """è·å–å®¢æˆ·ç«¯åˆ—è¡¨"""
        clients = self.client_manager.get_all_clients()
        return web.json_response({
            'clients': clients,
            'total_count': len(clients)
        })
    
    async def handle_heartbeat(self, request):
        """å¤„ç†å¿ƒè·³åŒ…"""
        try:
            data = await request.json()
            client_id = data.get('client_id')
            
            if not client_id:
                return web.json_response(
                    {'error': 'missing client_id'}, 
                    status=400
                )
            
            # æ³¨å†Œæˆ–æ›´æ–°å®¢æˆ·ç«¯
            if client_id not in self.client_manager.clients:
                self.client_manager.register_client(client_id)
            else:
                self.client_manager.update_heartbeat(client_id)
            
            self.stats['total_requests'] += 1
            
            return web.json_response({
                'status': 'ok',
                'server_time': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"âŒ å¿ƒè·³å¤„ç†é”™è¯¯: {e}")
            return web.json_response(
                {'error': str(e)}, 
                status=500
            )
    
    async def handle_single_activity(self, request):
        """å¤„ç†å•ä¸ªæ´»åŠ¨æ•°æ®"""
        try:
            data = await request.json()
            activity = ActivityData.from_dict(data)
            
            # æ³¨å†Œå®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if activity.client_id not in self.client_manager.clients:
                self.client_manager.register_client(activity.client_id)
            
            # æ·»åŠ æ´»åŠ¨
            success = self.client_manager.add_activities(activity.client_id, [activity])
            
            if success:
                self.stats['total_activities'] += 1
                return web.json_response({'status': 'received'})
            else:
                return web.json_response(
                    {'error': 'failed to store activity'}, 
                    status=500
                )
                
        except Exception as e:
            logger.error(f"âŒ å•ä¸ªæ´»åŠ¨å¤„ç†é”™è¯¯: {e}")
            return web.json_response(
                {'error': str(e)}, 
                status=500
            )
    
    async def handle_batch_activities(self, request):
        """å¤„ç†æ‰¹é‡æ´»åŠ¨æ•°æ®"""
        try:
            data = await request.json()
            client_id = data.get('client_id')
            activities_data = data.get('activities', [])
            
            if not client_id:
                return web.json_response(
                    {'error': 'missing client_id'}, 
                    status=400
                )
            
            # è½¬æ¢ä¸ºActivityDataå¯¹è±¡
            activities = [ActivityData.from_dict(act) for act in activities_data]
            
            # æ³¨å†Œå®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if client_id not in self.client_manager.clients:
                self.client_manager.register_client(client_id)
            
            # æ·»åŠ æ´»åŠ¨
            success = self.client_manager.add_activities(client_id, activities)
            
            if success:
                self.stats['total_activities'] += len(activities)
                logger.info(f"ğŸ“¦ æ”¶åˆ°æ‰¹é‡æ´»åŠ¨: {client_id} - {len(activities)} ä¸ª")
                
                return web.json_response({
                    'status': 'received',
                    'count': len(activities)
                })
            else:
                return web.json_response(
                    {'error': 'failed to store activities'}, 
                    status=500
                )
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ´»åŠ¨å¤„ç†é”™è¯¯: {e}")
            return web.json_response(
                {'error': str(e)}, 
                status=500
            )
    
    async def handle_prediction_request(self, request):
        """å¤„ç†é¢„æµ‹è¯·æ±‚"""
        try:
            data = await request.json()
            pred_request = PredictionRequest.from_dict(data)
            
            # è·å–å®¢æˆ·ç«¯æœ€è¿‘çš„æ´»åŠ¨ï¼ˆå¦‚æœè¯·æ±‚ä¸­æ²¡æœ‰æä¾›è¶³å¤Ÿçš„æ´»åŠ¨ï¼‰
            if len(pred_request.activities) < 3:
                client_activities = self.client_manager.get_client_activities(
                    pred_request.client_id, 20
                )
                if client_activities:
                    pred_request.activities = client_activities
            
            if len(pred_request.activities) < 1:
                return web.json_response(
                    {'error': 'insufficient activity data'}, 
                    status=400
                )
            
            # æ‰§è¡Œé¢„æµ‹
            prediction_result = await self.llm_predictor.predict_next_activity(
                pred_request.activities
            )
            
            # åˆ›å»ºå“åº”
            response = PredictionResponse(
                request_id=pred_request.request_id,
                prediction=prediction_result['prediction'],
                confidence=prediction_result['confidence'],
                timestamp=datetime.now().isoformat(),
                processing_time=prediction_result['processing_time']
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_predictions'] += 1
            self.client_manager.increment_prediction_count(pred_request.client_id)
            
            logger.info(f"ğŸ”® é¢„æµ‹å®Œæˆ: {pred_request.client_id} - {response.prediction[:50]}...")
            
            return web.json_response(response.to_dict())
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¤„ç†é”™è¯¯: {e}")
            return web.json_response(
                {'error': str(e)}, 
                status=500
            )
    
    async def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        logger.info("ğŸš€ å¯åŠ¨æ´»åŠ¨é¢„æµ‹æœåŠ¡å™¨")
        
        self.stats['start_time'] = datetime.now().isoformat()
        
        # åŠ è½½LLMæ¨¡å‹
        logger.info("ğŸ“¥ åŠ è½½LLMæ¨¡å‹...")
        model_loaded = await self.llm_predictor.load_model()
        if not model_loaded:
            logger.warning("âš ï¸ LLMæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿé¢„æµ‹å™¨")
        
        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        # å¯åŠ¨webæœåŠ¡å™¨
        runner = web.AppRunner(self.app)
        await runner.setup()
        
        site = web.TCPSite(runner, self.config.host, self.config.port)
        await site.start()
        
        logger.info(f"âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ - http://{self.config.host}:{self.config.port}")
        logger.info(f"ğŸ“‹ APIç«¯ç‚¹:")
        logger.info(f"   å¥åº·æ£€æŸ¥: GET /health")
        logger.info(f"   æœåŠ¡å™¨çŠ¶æ€: GET /status")
        logger.info(f"   å®¢æˆ·ç«¯åˆ—è¡¨: GET /clients")
        logger.info(f"   å¿ƒè·³: POST /heartbeat")
        logger.info(f"   æ´»åŠ¨æ•°æ®: POST /activities/batch")
        logger.info(f"   é¢„æµ‹è¯·æ±‚: POST /predict")
        
        return runner, cleanup_task
    
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                self.client_manager.cleanup_inactive_clients()
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†å¾ªç¯é”™è¯¯: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Linuxæ´»åŠ¨é¢„æµ‹æœåŠ¡å™¨")
    parser.add_argument("--config", "-c", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--host", help="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    parser.add_argument("--port", "-p", type=int, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--model", "-m", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora", "-l", help="LoRAæƒé‡è·¯å¾„")
    parser.add_argument("--no-gpu", action="store_true", help="ç¦ç”¨GPU")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæœåŠ¡å™¨é…ç½®
    config = ServerConfig(args.config)
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if args.host:
        config.host = args.host
    if args.port:
        config.port = args.port
    if args.model:
        config.model_path = args.model
    if args.lora:
        config.lora_path = args.lora
    if args.no_gpu:
        config.enable_gpu = False
    
    # åˆ›å»ºæœåŠ¡å™¨
    server = ActivityPredictionServer(config)
    
    try:
        # å¯åŠ¨æœåŠ¡å™¨
        runner, cleanup_task = await server.start_server()
        
        print("\nğŸ”„ æœåŠ¡å™¨è¿è¡Œä¸­... æŒ‰ Ctrl+C åœæ­¢")
        
        # ä¿æŒè¿è¡Œç›´åˆ°ä¸­æ–­
        while True:
            await asyncio.sleep(1)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨è¿è¡Œé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if 'cleanup_task' in locals():
            cleanup_task.cancel()
        if 'runner' in locals():
            await runner.cleanup()
        
        logger.info("âœ… æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    asyncio.run(main()) 