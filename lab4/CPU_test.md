# 运行命令
```r
 ./llama-batched-bench -m Qwen3-8B-UD-Q5_K_XL.gguf -c 2048 -b 2048 -ub 512 -npp 128,256,512 -ntg 128,256 -npl 1,2,4,8,16,32
```

## 参数解释


1. -m 参数后添加模型的gguf文件所在位置
```r
-m,    --model FNAME 
```

2. -c 提示文本的大小
```r
-c,    --ctx-size N
```

3. -b 每次模型前向推理时最多可以同时处理的 token 数量（即 batch 大小）
```r
-b,    --batch-size N 
```

4. -ub 一次实际可并行处理的 token 数量
```r
-ub,   --ubatch-size N
```

**注意：-b 与 -ub的区别**
| 名称                              | 含义                                                 | 默认值  | 作用域         |
| ------------------------------- | -------------------------------------------------- | ---- | ----------- |
| **Logical Maximum Batch Size** （-b） | 调用 `llama_eval()` 时 **最多允许请求处理的 token 数量**（用户设定） | 2048 | 用户侧限制（软件逻辑） |
| **Physical Maximum Batch Size**（-ub） | 实际为推理时分配的 **底层缓存容量（token buffer）**，是硬件资源限制         | 512  | 系统侧限制（内存限制） |

5. -npp 指定每组 benchmark 测试中 prompt token 的数量，即输入给模型的 token 数
```r
-npp n0,n1,...                          number of prompt tokens
```

6. -ntg 每次测试中要生成的 token 数量
```r
-ntg n0,n1,...                          number of text generation tokens
```

7. -npl 同时处理的 prompt 数量，也就是模拟**并发推理**的负载场景。
```r
-npl n0,n1,...                          number of parallel prompts
```

# 结果分析
## 输出结果
|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|   128 |    128 |    1 |    256 |    5.432 |    23.56 |   14.804 |     8.65 |   20.237 |    12.65 |
|   128 |    128 |    2 |    512 |   11.509 |    22.24 |   18.623 |    13.75 |   30.132 |    16.99 |
|   128 |    128 |    4 |   1024 |   23.688 |    21.61 |   30.684 |    16.69 |   54.373 |    18.83 |
|   128 |    128 |    8 |   2048 |   46.589 |    21.98 |   62.923 |    16.27 |  109.512 |    18.70 |
|   128 |    256 |    1 |    384 |    6.956 |    18.40 |   41.796 |     6.12 |   48.753 |     7.88 |
|   128 |    256 |    2 |    768 |   12.185 |    21.01 |   40.820 |    12.54 |   53.006 |    14.49 |
|   128 |    256 |    4 |   1536 |   24.453 |    20.94 |   73.378 |    13.96 |   97.831 |    15.70 |
|   256 |    128 |    1 |    384 |   13.268 |    19.30 |   17.380 |     7.36 |   30.647 |    12.53 |
|   256 |    128 |    2 |    768 |   25.730 |    19.90 |   20.217 |    12.66 |   45.948 |    16.71 |
|   256 |    128 |    4 |   1536 |   50.451 |    20.30 |   34.241 |    14.95 |   84.692 |    18.14 |
|   256 |    256 |    1 |    512 |   11.802 |    21.69 |   30.452 |     8.41 |   42.254 |    12.12 |
|   256 |    256 |    2 |   1024 |   24.238 |    21.12 |   40.653 |    12.59 |   64.892 |    15.78 |
|   256 |    256 |    4 |   2048 |   49.172 |    20.82 |   69.770 |    14.68 |  118.943 |    17.22 |
|   512 |    128 |    1 |    640 |   27.375 |    18.70 |   20.035 |     6.39 |   47.410 |    13.50 |
|   512 |    128 |    2 |   1280 |   48.384 |    21.16 |   19.814 |    12.92 |   68.197 |    18.77 |
|   512 |    256 |    1 |    768 |   22.862 |    22.40 |   28.428 |     9.01 |   51.290 |    14.97 |
|   512 |    256 |    2 |   1536 |   47.350 |    21.63 |   41.145 |    12.44 |   88.494 |    17.36 |

llama_perf_context_print:        load time =    5184.35 ms
llama_perf_context_print: prompt eval time =  904505.37 ms / 15888 tokens (   56.93 ms per token,    17.57 tokens per second)
llama_perf_context_print:        eval time =  152890.51 ms /  1152 runs   (  132.72 ms per token,     7.53 tokens per second)
llama_perf_context_print:       total time = 1061798.55 ms / 17040 tokens

## 结果分析

| 字段名        | 含义                                       |
| ---------- | ---------------------------------------- |
| `PP`       | Prompt Tokens：每个输入样本的 prompt 长度（token 数） |
| `TG`       | Text Generation Tokens：每个样本生成的 token 数   |
| `B`        | Batch Size：同时推理的输入样本数（并发）                |
| `N_KV`     | KV Cache 数量 = (PP + TG) × B              |
| `T_PP s`   | Prompt 处理时间（秒）                           |
| `S_PP t/s` | Prompt 阶段吞吐率（tokens/s）                   |
| `T_TG s`   | 生成阶段耗时（秒）                                |
| `S_TG t/s` | 生成阶段吞吐率                                  |
| `T s`      | 总时间（秒）                                   |
| `S t/s`    | 总体吞吐率（PP+TG×B 的 token/s）                 |

### Q1: KV-Cache(N_KV)的数量为什么是（PP+TG）*B？
>原因分析：
>
>1. 每个 token 会在每一层都生成一个 K 和一个 V 向量，这些都要缓存下来：
**Prompt 阶段（PP）**：一开始输入的所有 token 都会立即计算 K/V 并保存；
>**生成阶段（TG）**：每生成一个 token，也会生成一组 K/V 并加入缓存；所以总共是 PP + TG 个 token 要缓存。
>2. 每个 batch 是独立的会话（序列）。Batch size 为 B 时，表示并发处理 B 个独立的序列；每个序列都要维护自己的上下文（KV-Cache）；所以缓存量要乘以 B，得到最终的：
KV-Cache 总长度 = (Prompt token 数 + 生成 token 数) × Batch 数
               = (PP + TG) × B

### Q2: 📈 为什么 T_PP 随 PP 线性增长？

>原因分析：
每个 prompt token 都需要前向推理 + 缓存写入。
>
>- KV-Cache 增加 →token 数增多；
>- 每个 token 必须：
1.过所有 transformer 层；
2.在每层生成并写入一对 K/V 向量；
>- 这些操作全都需要时间，且是线性累加的。
>因此，token 越多 → 推理时间越长 → T_PP 线性增长

### Q3: 📉 为什么 S_PP 基本维持不变？
>⚠️ **原因**：吞吐率 = token 数（PP） ÷ 时间（T_PP）。Token数主要由输入的Token数（PP）衡量，T_PP又随着Token数线性增长，两者作除相当于对Prompt阶段吞吐率没有显著影响。


### Q4: 🔍T_TG/S_TG的变化受哪些因素影响？
✅ **1. 生成 token 数（TG）**
>TG 越多 → 要执行的推理步骤越多（每生成一个 token 都是一次完整的 forward pass）；所以 T_TG 上线性增长。但 S_TG却不是线性增长，因为生成 token 会不断增长 context 长度，导致每步越来越慢，S_TG的增长比线性增长慢。


✅ **2. Prompt长度**
>PP 越大 → KV-cache 越长，每生成一个 token 时，要和所有已有 token 做 attention。所以，PP 越大，每个生成 token 的计算成本也越大 → T_TG 变慢 → S_TG 降低。


✅ **3. Batch Size大小**
从理论上来说，增加Batch Size大小，应该会让生成的Token数线性增长。然而我们在实验中发现如果只增加batch的情况下，T_TG呈现出一开始“亚线性增长”，后来变为恶化的线性增长的情况。结果如下图所示。


| Batch (B) | T\_TG (秒) | 备注              |
| --------- | --------- | --------------- |
| 1         | 14.80     | baseline        |
| 2         | 18.62     | 增加 1.25 倍 ≪ 2 倍 |
| 4         | 30.68     | 增加约 2 倍 ≪ 4 倍   |
| 8         | 62.92     | 增加约 4 倍 ≈ 8 倍   |

>⚠️ **原因**：
>1. **Transformer架构具备高度的并行性**
由于每条序列（batch 内）在 GPU 上是并行执行的，注意力、MLP、KV-cache 读写都能 在 batch 内并发处理，所以前期 batch 扩展不会线性增加耗时，而是摊薄了计算。
>2. **数据复用 & 硬件流水线摊薄初始化开销**
多个序列共享：模型参数、缓存读取、计算图构建等，能让 GPU / CPU pipeline 更高效。
因此，初始化 overhead 可以被摊薄。
>3. **并行线程占满但是未溢出**
小 batch（如 1 → 2 → 4）时，资源还未饱和，
系统能把新增 workload 平均分配到多个线程/核心，所以 T_TG 增幅是亚线性。
>4. **资源饱和点后才进入线性甚至次线性**
当 batch 扩展到系统内存/GPU 核心极限时，计算排队/缓存 thrashing 开始出现。这时 T_TG 会开始线性或指数级增长。典型表现：L2 cache miss 频繁；KV-cache 拆页；GPU warp thread 饱和。

>由于T_TG不会线性增加，因此S_TG会随Batch Size的增大而增大，即Batch Size能够提高并发度，提升整体吞吐率。

✅ **4. KV Cache 命中率 & 读写速度**
> 由于生成阶段全靠读取 KV-cache（key/value 向量），如果 cache 局部性差，频繁 miss，会严重拖慢生成速度。另外，尤其在CPU模式下，大 KV-cache 也可能导致内存访问拥堵。


### Q5: 🔍T/S的如何变化？
```r
T = T_PP + T_TP
```
输入和输出的总时间取决于T_PP和T_TP。由于T_PP基本不变，因此可以认为T主要取决于T_TP。



```r
S = N_KV ÷ T
```
总吞吐量计算方式如上，它取决于N_KV与T的变化。
![alt text](/lab4/assets/S.png)

✅ **1. 随着 Batch Size (B) 增加，S 增长快速但趋于饱和**
| Batch Size | S t/s 示例 |
| ---------- | -------- |
| 1          | 12\~14   |
| 2          | 16\~18   |
| 4          | 18\~19   |
| 8          | \~18     |
>小 batch 时，增加 batch 能显著提升吞吐率。原因为并行效率提升、初始化成本被摊薄。
但随着 batch 增大，GPU、内存带宽以及L2 cache 等资源趋于饱和，吞吐率增幅下降甚至趋于平稳。

✅ **2. Prompt token (PP) 越长，S 趋势下降或维持持平**
因为 prompt 处理是一次性、无法并行的高负载计算。长 prompt 会增加 KV-Cache 初始化成本，占用大量计算资源；所以当 PP 很大（如 512）时，即使 batch 增大，S 提升也较小。

✅ **3. Generation token (TG) 增加时，S 初期上升、后期下降**

| TG 增加趋势       | 吞吐变化    | 原因说明           |
| ------------- | ------- | -------------- |
| 128 → 256     | 吞吐可能上升  | 启动成本被摊薄、上下文尚短  |
| 256 → 512（假设） | 吞吐下降或停滞 | 上下文越来越长、注意力开销大 |

>Transformer 生成是顺序的，每多一个 token，都要与之前全部 token 做 attention。所以 TG 增大导致计算呈现二次增长（O(n²)）趋势，最终使得吞吐率下降。

### 首Token延迟分析
“首 Token 延迟”指的是从开始输入 prompt，到第一个输出 token 被生成出来之间的时间。这个是衡量响应速度最重要的指标之一，尤其在交互式场景中。计算这一延迟时间需要使用到如下两个变量。
1. Prompt 评估时间（prompt eval time）：输入的 prompt（即模型的上下文内容）中，15888 个 token 的推理耗时总计 904505.37 毫秒。
2. 生成阶段的推理时间（eval time）：模型生成输出 token 的评估时间（即 decoder 阶段），共生成了 1152 个 token。每个生成 token 平均耗时 132.72 毫秒，吞吐量 7.53 tokens/s

根据实验获得的数据：
```r
first_token_latency = prompt eval time + 1st token eval time

其中，Prompt eval time ≈ 904505.37 ms

First token eval time ≈ 132.72 ms（因为 avg per token）

所以，首 Token 延迟 ≈ 904505.37 + 132.72 = 904638.09 ms(约为15分钟)
```











